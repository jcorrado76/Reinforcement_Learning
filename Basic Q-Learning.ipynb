{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the World\n",
    "***\n",
    "In a Reinforcement Learning task, one trains an _agent_ that interacts with an _environment_. The agent transitions between different _states_ in the environment by performing _actions_. Actions yield _rewards_ that can be positive, negative or zero. The agent's goal is to maximize the total reward over an _episode_. An agent develops a _policy_ trying certain actions, and being reinforced with positive rewards, or punished with negative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "***\n",
    "Each state the agent is in can be modeled as a consequence of the previous state and the chosen action. However, every step also depends on the entire history of actions and states taken. In order to simplify the computation of the next best action, we model the states as **markov states**. And so, the probability of being in any state depends only on the one that came before it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation\n",
    "***\n",
    "Assume you can write down the expected reward for every action, at any step. How do we choose what the next action should be? Well, you should choose the sequence of actions that, on average, generates the highest reward. We call the cumulative reward after one episode a _Q Value_ (short for _quality value_). This is summarized by the **Bellman Equation**, which states:\n",
    "$$Q(s,a) = r(s,a) +\\gamma \\mathrm{max}_aQ(s',a)$$\n",
    "So the next action would be:\n",
    "$$\\hat{a}=\\mathrm{argmax}_aQ(s,a)$$\n",
    "The Bellman Equation states that the value added from selecing action $a$ while in state $s$ is equal to the immediate reward $r(s,a)$, plus the value you would get from moving into state $s'$ after selecting action $a$.\n",
    "\n",
    "Now it becomes clear why we wrote this as an optimization problem: we want to maximize the value added by selecting the action that maximizes the additional quality gained after moving into any accessible state. \n",
    "\n",
    "The $\\gamma$ is the _discount factor_. This fraction is present to penalize the agent from thinking ahead. It runs between 0 and 1. If $\\gamma=1$, then we consider the potential rewards from all future steps equally. If $\\gamma=0$, we only consider the next immediate reward (extreme short-sightedness). This factor is introduced to combat the so-called exploration versus exploitation tradeoff. $\\gamma=0$ corresponds to extreme exploitation (of current knowledge and only chooses actions to maximize immediate rewards). $\\gamma=1$ corresponds to extreme exploration, and the agent considers all the possible future rewards in the determination of the next step. \n",
    "\n",
    "You need your agent to explore, because certain states at later steps might yield higher rewards as a result of exploring more of the state space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "***\n",
    "At any given state, the agent shoud perform the action that will eventually yield the highest cumulative reward. Algorithms like this are called _greedy_. One way to implement this in real life is to store all possible state-action combinations. We use this matrix to save the Q values, and we update it using the Bellman Equation as an update rule. \n",
    "\n",
    "You can imagine that this becomes expensive as you are computing a cartesian product between the action space and the state space. \n",
    "\n",
    "If your future state is a terminal state, you will be left with:\n",
    "$$Q(s,a)=r(s,a)$$\n",
    "\n",
    "\n",
    "In order to implement a solution ot the exploitation versus exploration problem, we use the so-called _$\\epsilon$-greedy_ approach. For some fraction $0<\\epsilon<1$, we choose the greedy action from the table with probability $p=1-\\epsilon$ or we choose a random action with probability $p=\\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "***\n",
    "Q learning scales really poorly, as noted above. As the number of states and actions becomes enormous, the cartesian product explodes. \n",
    "\n",
    "In order to solve complex problems, we combine Q learning with Deep Learning, yielding _Deep Q Networks_. We replace the Q Learning table with a neural network to try and approximate Q values. The function represented by the neural network is now referred to as the _approximating function_, denoted by $Q(s,a;\\theta)$, where $\\theta$ represents the weights of the neural network. \n",
    "\n",
    "Now, we interpret the Bellman Equation as the cost function.\n",
    "$$Q(s,a) = r(s,a)+\\gamma \\mathrm{max}_aQ(s',a)$$\n",
    "\n",
    "This is usually interpreted as a rule for updating the Q values, but it will be satisfied as an equality precisely when the Q Table has converged to its final values. So, what we want to minimize in our implementation will be the difference between the left hand side and the right hand sides of the Bellman Equation: (The DQN Cost function)\n",
    "$$J(\\theta)=\\left(Q(s,a;\\theta)-\\left(r(s,a)+\\gamma\\mathrm{max}_aQ(s',a;\\theta)\\right)\\right)^2$$\n",
    "\n",
    "This should remind you of the mean squared error cost function, where the current Q value is the analogy of the true label y, and the immediate + future rewards terms are the analogy of the predicted values for the labels. \n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "***\n",
    "The characteristic difference between RL and SL is that the training set is created as we go. We ask the agent to try and select the best action using the current network, and we record the state, action, reward as well as the next state it ended up in. \n",
    "\n",
    "So we then decide on a batch size, and after every time another batch worth of records is created, we select a batch of records at random from the data store, and train the network. \n",
    "\n",
    "These memory buffers are usually referred to as _Experience Replay_. Several types of these memories exist; a common one is _cyclic memory buffer_. This one makes sure the agent keeps training over its new behavior rather than things that might no longer be relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Learning\n",
    "***\n",
    "In SL, MSE compares the true labels to predicted labels, and the true labels are considered to be constant throughout training. In Deep Q networks this is not the case. Both $y$ and $\\hat{y}$ are being predicted in by the network itself and will vary at every iteration. \n",
    "\n",
    "The **Double Deep Q Network** uses semi-constant labels during training by creating two copies of the Q Network, and updating only one of them. After some number of iterations, the constant network is replaced with a copy of the updated Q Network, and we minimize the cost over these semi-constant weights ($\\tau$) instead. \n",
    "$$J(\\theta) = \\left[Q(s,a;\\theta)-\\left(r(s,a)+\\gamma \\mathrm{max}_aQ(s',a;\\tau\\right)\\right]^2$$\n",
    "Here, $Q(s',a'\\tau)$ represents the Q value predicted by the semi-constant network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Example\n",
    "***\n",
    "As an example, we are going to try to implement a simple idea for a 1D board game. There are 4 states on the board. When the agent selects a empty slot, it receives a reward of 1, and the slot becomes filled. If it selects a non-vacant slow, it receives a reward of -1. The game is over when the entire board is full \n",
    "\n",
    "This is equivalent to a +1 reward every time it fills a vacant cell, and a -1 penalty when it tries to fill an already filled cell. The game ends when the board is full. \n",
    "\n",
    "The possible states for this board game are represented by the set of vectors in $\\{0,1\\}^4$ that represent each of the ways the states on the board can be filled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write an ```Agent``` class to hold the q-table, a ```Board``` class for holding the state of the grid (environment), and a ```Simulation``` class to manage the learning history and sequence of games the agent will play.\n",
    "\n",
    "Note, the Agent is responsible for keeping track of its rewards, its q-table, updating the q table appropriately, and selecting what action to take. The Board is responsible for evaluating the actions the agent took, and rewarding the agent appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     1,
     11,
     22,
     23,
     39,
     42,
     45,
     51,
     54,
     82,
     88,
     95,
     105,
     109,
     147,
     148,
     159
    ]
   },
   "outputs": [],
   "source": [
    "# a function to convert np array into string state representation\n",
    "def print_state(state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    state - np array containing grid\n",
    "    Outputs:\n",
    "    string representation of the list to use for column labels\n",
    "    \"\"\"\n",
    "    return(str(list(map(int, state.tolist()))))\n",
    "\n",
    "# update equation\n",
    "def bellman(max_q, reward, gamma=1.):\n",
    "    \"\"\"\n",
    "    Compute the bellman equation for update of Q-table\n",
    "    Inputs:\n",
    "    max_q - (float) maximum quality to be gained in next state\n",
    "    reward - (float) reward to be gained in next state\n",
    "    gamma - discount factor\n",
    "    \"\"\"\n",
    "    return reward + gamma * max_q\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, possible_states):\n",
    "        \"\"\"\n",
    "        Initialize the agent, its memory and rewards\n",
    "\n",
    "        Inputs:\n",
    "        possible_states - [list] of string representations of possible\n",
    "                          states\n",
    "        \"\"\"\n",
    "        self.total_reward = 0\n",
    "        self.reward_hist = []\n",
    "        self.q_table = pd.DataFrame(0, index=np.arange(4),\n",
    "                                    columns=possible_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def clear_reward_hist( self ):\n",
    "        self.reward_hist = []\n",
    "\n",
    "    def update_reward(self, reward):\n",
    "        self.total_reward += reward\n",
    "\n",
    "    def update_reward_hist(self):\n",
    "        \"\"\"\n",
    "        Append cumulative reward at end of episode\n",
    "        \"\"\"\n",
    "        self.reward_hist.append(self.total_reward)\n",
    "        \n",
    "    def get_reward_hist(self):\n",
    "        return self.reward_hist\n",
    "\n",
    "    def choose_action(self, old_state, epsilon=0.5):\n",
    "        \"\"\"\n",
    "        The environment passes the current state to the agent.\n",
    "        The agent chooses an action to take from that state\n",
    "        and returns it to the environment.\n",
    "        \n",
    "        As we see, the actions are either chosen by the randomness,\n",
    "        or by taking the row whose action had the maximal q value \n",
    "        for that current state.\n",
    "        \n",
    "        Take the column for the current state, and select its row\n",
    "        with the highest entry. That is the action to take. \n",
    "\n",
    "        Inputs:\n",
    "        state - np array containing state of environment\n",
    "        epsilon - epsilon greedy probability\n",
    "        Outputs:\n",
    "        action - [int] corresponding to row number to try to fill\n",
    "        \"\"\"\n",
    "        # e-greedy. choose random action with probability epsilon\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        # with probability 1-epsilon choose action to be argmax of\n",
    "        # learned q-table\n",
    "        else:\n",
    "            action = self.q_table[print_state(old_state)].idxmax()\n",
    "        return action\n",
    "\n",
    "    def get_max_q_value(self, new_state):\n",
    "        if np.sum(new_state) == 4:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.q_table[print_state(new_state)].max()\n",
    "\n",
    "    def update_q_table(self, action, reward, old_state, new_state, gamma=1.0):\n",
    "        # compute max q based on new state\n",
    "        new_max_q_value = self.get_max_q_value(new_state)\n",
    "        # update Q table at old state\n",
    "        self.q_table.loc[action, print_state(old_state)] = bellman(\n",
    "            new_max_q_value, reward, gamma=gamma)\n",
    "        \n",
    "    def show_table(self):\n",
    "        \"\"\"\n",
    "        Print the final learned q-table. Note, the row with the\n",
    "        maximum q-value in each of the columns will correspond to\n",
    "        the action to take while in that state\n",
    "        \"\"\"\n",
    "        print(\"Final Q-table:\\n\")\n",
    "        print(self.q_table)\n",
    "\n",
    "\n",
    "class Board(object):\n",
    "    board = None\n",
    "    board_size = 0\n",
    "\n",
    "    def __init__(self, board_size=4):\n",
    "        \"\"\"\n",
    "        Initialize board size and reset game \n",
    "\n",
    "        Inputs:\n",
    "        board_size - tuple containing board size. passed to np.zeros\n",
    "        Outputs:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Set entire board to zeros\n",
    "        \"\"\"\n",
    "        self.board = np.zeros(self.board_size)\n",
    "\n",
    "    def evaluate(self, action):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        action - integer; represents the action. index of cell \n",
    "        to try to fill \n",
    "\n",
    "        Outputs:\n",
    "        tuple (reward, game_over?)\n",
    "        \"\"\"\n",
    "        if self.board[action] == 0:\n",
    "            self.board[action] = 1\n",
    "            game_over = (self.board == 1).all()\n",
    "            return(1, game_over)\n",
    "        else:\n",
    "            return(-1, False)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board\n",
    "\n",
    "\n",
    "class Simulation(object):\n",
    "    def __init__(self, possible_states , number_games=2000, eps=0.1, gam=1.0):\n",
    "        assert (gam>=0.0 and gam<=1.0),\"Gamma must be between 0 and 1\"\n",
    "        assert (eps>=0.0 and eps<=1.0),\"Epsilon must be between 0 and 1\"\n",
    "        # initialize hyperparameters\n",
    "        self.number_episodes = number_games\n",
    "        self.epsilon = eps\n",
    "        self.gamma = gam\n",
    "        # initialize board and agent objects\n",
    "        self.board = Board()\n",
    "        self.agent = Agent(possible_states)\n",
    "\n",
    "    def play(self):\n",
    "        for game_number in range(self.number_episodes):\n",
    "            game_over = False\n",
    "            # reset the grid values\n",
    "            self.board.reset()\n",
    "            # reset total_reward to 0\n",
    "            self.agent.reset()\n",
    "            while not game_over:\n",
    "                old_state = np.copy(self.board.get_state())\n",
    "                action = self.agent.choose_action(old_state,\n",
    "                            epsilon=self.epsilon)\n",
    "                # UPDATE STATE and get reward\n",
    "                reward, game_over = self.board.evaluate(action)\n",
    "                # get new state\n",
    "                new_state = np.copy(self.board.get_state())\n",
    "                # increment total reward acquired from action\n",
    "                self.agent.update_reward(reward)\n",
    "                # update Q table\n",
    "                self.agent.update_q_table( action, reward,\n",
    "                        old_state, new_state, gamma=self.gamma)\n",
    "\n",
    "            # add total reward to reward_hist\n",
    "            self.agent.update_reward_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "States:\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 1, 0]\n",
      "[0, 0, 1, 1]\n",
      "[0, 1, 0, 0]\n",
      "[0, 1, 0, 1]\n",
      "[0, 1, 1, 0]\n",
      "[0, 1, 1, 1]\n",
      "[1, 0, 0, 0]\n",
      "[1, 0, 0, 1]\n",
      "[1, 0, 1, 0]\n",
      "[1, 0, 1, 1]\n",
      "[1, 1, 0, 0]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# let's print out all the possible states\n",
    "possible_states = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                state = np.array([i,j,k,l])\n",
    "                possible_states.append( print_state(state) )\n",
    "print(\"Number of states: {}\".format(len(possible_states)))\n",
    "print(\"States:\")\n",
    "for state in possible_states:\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, as expected, the number of states is $2^4=16$\n",
    "\n",
    "Now, let's train our agent using $\\epsilon=0.1$ and $\\gamma=1.0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[0, 0, 0, 0]</th>\n",
       "      <th>[0, 0, 0, 1]</th>\n",
       "      <th>[0, 0, 1, 0]</th>\n",
       "      <th>[0, 0, 1, 1]</th>\n",
       "      <th>[0, 1, 0, 0]</th>\n",
       "      <th>[0, 1, 0, 1]</th>\n",
       "      <th>[0, 1, 1, 0]</th>\n",
       "      <th>[0, 1, 1, 1]</th>\n",
       "      <th>[1, 0, 0, 0]</th>\n",
       "      <th>[1, 0, 0, 1]</th>\n",
       "      <th>[1, 0, 1, 0]</th>\n",
       "      <th>[1, 0, 1, 1]</th>\n",
       "      <th>[1, 1, 0, 0]</th>\n",
       "      <th>[1, 1, 0, 1]</th>\n",
       "      <th>[1, 1, 1, 0]</th>\n",
       "      <th>[1, 1, 1, 1]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   [0, 0, 0, 0]  [0, 0, 0, 1]  [0, 0, 1, 0]  [0, 0, 1, 1]  [0, 1, 0, 0]  \\\n",
       "0           4.0           3.0           3.0           2.0           3.0   \n",
       "1           4.0           0.0           1.0           0.0           2.0   \n",
       "2           4.0           3.0           2.0           0.0           3.0   \n",
       "3           4.0           2.0           3.0           0.0           1.0   \n",
       "\n",
       "   [0, 1, 0, 1]  [0, 1, 1, 0]  [0, 1, 1, 1]  [1, 0, 0, 0]  [1, 0, 0, 1]  \\\n",
       "0           2.0           2.0             0           2.0           1.0   \n",
       "1           0.0           0.0             0           3.0           2.0   \n",
       "2           0.0           0.0             0           3.0           2.0   \n",
       "3           0.0           0.0             0           3.0           1.0   \n",
       "\n",
       "   [1, 0, 1, 0]  [1, 0, 1, 1]  [1, 1, 0, 0]  [1, 1, 0, 1]  [1, 1, 1, 0]  \\\n",
       "0           1.0          -1.0           1.0           0.0           0.0   \n",
       "1           2.0           1.0           1.0           0.0           0.0   \n",
       "2           1.0           0.0           2.0           1.0           0.0   \n",
       "3           2.0           0.0           2.0           0.0           1.0   \n",
       "\n",
       "   [1, 1, 1, 1]  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = Simulation(possible_states,number_games=4000, eps=0.1, gam=1.0)\n",
    "sim.play()\n",
    "sim.agent.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the agent has learned a policy. The Q-table should be interpreted by selecting the column corresponding to the current state. Then, the row with the highest entry will be the action to take from that state. We see that our agent has successfully learned a decent policy.\n",
    "\n",
    "We can check this programmatically by looping over the possible states and showing what the agent would have done while in each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [0 0 0 0]\tpredicted Q values: [4.0, 4.0, 4.0, 4.0] \taction: 0\tcorrect action? True\n",
      "state: [0 0 0 1]\tpredicted Q values: [3.0, 3.0, 3.0, 2.0] \taction: 0\tcorrect action? True\n",
      "state: [0 0 1 0]\tpredicted Q values: [3.0, 3.0, 2.0, 3.0] \taction: 0\tcorrect action? True\n",
      "state: [0 0 1 1]\tpredicted Q values: [2.0, 0.0, 1.0, 0.0] \taction: 0\tcorrect action? True\n",
      "state: [0 1 0 0]\tpredicted Q values: [3.0, 2.0, 3.0, 3.0] \taction: 0\tcorrect action? True\n",
      "state: [0 1 0 1]\tpredicted Q values: [2.0, -1.0, 1.0, 0.0] \taction: 0\tcorrect action? True\n",
      "state: [0 1 1 0]\tpredicted Q values: [2.0, 0.0, 0.0, 0.0] \taction: 0\tcorrect action? True\n",
      "state: [0 1 1 1]\tpredicted Q values: [1.0, 0.0, 0.0, 0.0] \taction: 0\tcorrect action? True\n",
      "state: [1 0 0 0]\tpredicted Q values: [2.0, 3.0, 3.0, 3.0] \taction: 1\tcorrect action? True\n",
      "state: [1 0 0 1]\tpredicted Q values: [1.0, 2.0, 2.0, 1.0] \taction: 1\tcorrect action? True\n",
      "state: [1 0 1 0]\tpredicted Q values: [1.0, 2.0, 1.0, 2.0] \taction: 1\tcorrect action? True\n",
      "state: [1 0 1 1]\tpredicted Q values: [-1.0, 1.0, 0.0, 0.0] \taction: 1\tcorrect action? True\n",
      "state: [1 1 0 0]\tpredicted Q values: [1.0, 1.0, 2.0, 2.0] \taction: 2\tcorrect action? True\n",
      "state: [1 1 0 1]\tpredicted Q values: [0.0, 0.0, 1.0, 0.0] \taction: 2\tcorrect action? True\n",
      "state: [1 1 1 0]\tpredicted Q values: [0.0, 0.0, 0.0, 1.0] \taction: 3\tcorrect action? True\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                state = np.array([i,j,k,l])\n",
    "                if len(np.where(state == 0)[0]) != 0:\n",
    "                    # select the Q values for this state\n",
    "                    pred = q_table[print_state(state)].tolist()\n",
    "                    # select the action in that state agent would have taken\n",
    "                    action = q_table[print_state(state)].idxmax()\n",
    "                    print('state: {}\\tpredicted Q values: {p} \\taction: {a}\\tcorrect action? {s}'\n",
    "                          .format(state,p=pred,a=action,s=state[action]==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our agent will not make any mistakes while playing this game and using the policy it developed. \n",
    "\n",
    "Note that the action our agent would have taken corresponds to $\\hat{y}_i$, while the \"predicted\" Q-values take the place of the \"true\" labels ($y_i$) in a SL problem. This resonates with the fact that we don't _know_ the labels in this case, and that's why we're using RL. \n",
    "\n",
    "If you look at state $[0,0,1,0]$, we see that, while the agent would have chosen the correct action, not all of the _predicted_ Q values are entirely correct. For example, in this state, we need the agent to fill the second entry, but not the third. However, in the Q matrix, the second Q-value is actually lower than the third, which means the agent still determined that, in this state, it derives more value from filling the third entry than from filling the second entry. \n",
    "This is a result of that fact that the algorithm is greedy, and the agent probably didn't reach this state enough times to determine the priority amongst the other entries that needed to be filled. We could fix this by increasing our $\\epsilon$, or increasing the number of games. \n",
    "\n",
    "Let's take a look at the reward history for the games played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_hist = sim.agent.get_reward_hist()\n",
    "plt.plot(np.array(reward_hist)/4.0*100)\n",
    "plt.xlabel(\"Game Number\")\n",
    "plt.ylabel(\"Reward %\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
